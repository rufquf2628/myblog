[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability/Probability.html",
    "href": "posts/Probability/Probability.html",
    "title": "1. Probability Theory and Random Variable",
    "section": "",
    "text": "Probability theory is a one of the branches in mathematics that optimize of random phenomena. The word “Probability” is defined in mathematics that “The extent to wihch an event is likely to occur.” In real world, the probability is used in case of coin tossing, card game, and rolling dice.\nTherefore, probability is a possible outcome derived from a event. There are many probability models in theory: random variable, normal distribution, and standard deviation, etc."
  },
  {
    "objectID": "posts/Probability/Probability.html#types-of-random-variable",
    "href": "posts/Probability/Probability.html#types-of-random-variable",
    "title": "1. Probability Theory and Random Variable",
    "section": "Types of Random Variable",
    "text": "Types of Random Variable\n\nDiscrete Random Variable\nDiscrete random variable, \\(X\\), is a list of all its possible outcomes which contains \\(x_1,x_2,\\ldots\\). Discrete random variable is specified by probability \\(p_i = P[X = x_i ]\\), for \\(i = 0,1,2,\\ldots\\).\nFor the coin toss example, let’s consider we toss the coin three times. If discrete random variable \\(X\\) represents the number of heads during the tossing, \\(X\\) can be the values of \\(0,1,2,\\) or \\(3\\). The possibility of coins should be TTT, TTH, THT, HTT, THH, HTH, HHT, and HHH. By counting the number of heads, we can find the probabilities.\n\\(p_0 = P[X = x_0] = 1/8\\) \\(\\newline\\) \\(p_1 = P[X = x_1] = 3/8\\) \\(\\newline\\) \\(p_2 = P[X = x_2] = 3/8\\) \\(\\newline\\) \\(p_3 = P[X = x_3] = 1/8\\) \\(\\newline\\)\n\n\nProbability Mass Function (PMF)\nDiscrete random variable can be expressed with Probability Mass Function (PMF). It shows x axis as \\(x_i\\) and y axis as \\(P(X)\\). So, in our coin toss example, x axis would be the number of heads and y axis would be the probabilities of each x.\n\nimport matplotlib.pyplot as plt\n\nX = [0, 1, 2, 3]\nY = [1/8, 3/8, 3/8, 1/8]\n\nplt.bar(X, Y, width=0.3)\nplt.xticks([0, 1, 2, 3])\nplt.yticks([1/8,2/8,3/8,4/8],[r'$\\frac{1}{8}$',r'$\\frac{2}{8}$',r'$\\frac{3}{8}$',r'$\\frac{4}{8}$'])\nplt.xlabel(\"x, Number of Heads\")\nplt.ylabel(\"P(x), Probability of x\")\nplt.show()\n\n\n\n\n\n\nContinuous Random Variable\nContinuous Random Variable, \\(X\\), is a random variable that can only be countinuous values. Continuous values should be uncountable and interval of the real numbers. So, the variable such as height, weight, and temperature can be countinuous variables.\nContinuous random variable \\(X\\) can be defined with \\(f(x)\\) which is integrated of \\(X\\) in any interval.\n\\(P(a &lt; X &lt; b) = \\int_{a}^{b} f(x) \\,dx\\)\n\n\nProbability Density Function (PDF)\nFor the Continuous Random Variable, the Probability Density Function (PDF) is used to represent. Since the continuous random variable represents the range of uncountable numbers, the graph also represents the space of certain range of x.\n\n\n\npdf.png\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nsample = np.random.normal(loc=50, scale=5, size=1000)\nmean = np.mean(sample)\nstd = np.std(sample)\ndist = norm(mean, std)\n\nvalues = [value for value in range(30, 70)]\nprobailities = [dist.pdf(value) for value in values]\n\nplt.xlabel('x')\n\nplt.hist(sample, bins = 10, density=True)\nplt.plot(values, probailities)\nplt.show()"
  },
  {
    "objectID": "posts/Classification/ClassificationModel.html",
    "href": "posts/Classification/ClassificationModel.html",
    "title": "What is Classfication Model",
    "section": "",
    "text": "Classification is a supervised machine learning model that can predict the correct “label” of a given input data. Model is fully trained with the given dataset and corresponding label to it.\nIn this section, we build a model that can predict that a customer gonna purchase our product with its age and estimated salary. So, in this model, age and estimated salary gonna be an input data, and purchase gonna be a label."
  },
  {
    "objectID": "posts/Classification/ClassificationModel.html#k-nearest-neighbors-algorithm",
    "href": "posts/Classification/ClassificationModel.html#k-nearest-neighbors-algorithm",
    "title": "What is Classfication Model",
    "section": "K-Nearest Neighbors algorithm",
    "text": "K-Nearest Neighbors algorithm\nK-Nearest Neighbors (KNN) is one of the most basic classification algorithm. In the classification phase, a user defines a hyper-parameter “k” which is the number of nearest data of each points. Simply, the model predicts which label is assigned to the input data by looking “k” nearest points of the input data.\n\n\n\nKNN.png\n\n\nIn the figure above, k is 5. So, the new data point looks for 5 nearest points and predicts that is is blue label.\n\nImporting the libraries and dataset\n\nimport numpy as np\nimport pandas as pd\n\n\ndataset = pd.read_csv('../dataset/Social_Network_Ads.csv')\ndataset\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n...\n...\n...\n...\n\n\n395\n46\n41000\n1\n\n\n396\n51\n23000\n1\n\n\n397\n50\n20000\n1\n\n\n398\n36\n33000\n0\n\n\n399\n49\n36000\n1\n\n\n\n\n400 rows × 3 columns\n\n\n\nAs stated above, our dataset, “Social Network Ads” contains 400 data of customers age, estimated salary, and wheather they purchased or not.\nNow, we split our dataset into train and test set as we done in regression model.\n\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)\n\n\n\nFeature Scaling\nAs we seen in dataset, we have two input data includes age and estimated salary. Although age is ranged between 10 to 90, estimated salary is ranged between 10000 to 40000; the scale of two inputs are different.\nSo, we need to do feature scaling of these data into the range between [-2.0 to 2.0]. StandardScaler library in sklearn will be helpful to handle this.\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n\nTraining the KNN model\nNow, we train our KNN model with training dataset.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\nprint(classifier.predict(sc.transform([[30,87000]])))\n\n[0]\n\n\nWith our trained KNN model, we can predict a specific customer whose age is 30 and estimated salary is 87000 gonna purchase our product. Our model predicts that he don’t purchase our product.\n\n\nMaking the Confusion Matrix\nAlso, we can see how accurate our model is with the confusion matrix provided by sklearn.\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)\n\n[[64  4]\n [ 3 29]]\n\n\n0.93\n\n\nBy compareing our predicted test set result and actual result, we can get a accuracy score.\nOur model shows that it is 93 percent correct in our test set.\n\n\nVisualising the results\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\nC:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_38696\\1813449438.py:12: UserWarning:\n\n*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n\n\n\n\n\n\n\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\nC:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_38696\\2197279724.py:10: UserWarning:\n\n*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "myblog",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n:::{.list .quarto-listing-default}\n\n\n\n\n\nProbability Theory and Random Variable\n\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nWhat is Classfication Model\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nWhat is Clustering Model\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nWhat is Regressor Model\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\n1 min\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n\n\n\nProbability Theory and Random Variable\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\n\n\nWhat is Classfication Model\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nWhat is Clustering Model\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nWhat is Regressor Model\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "What is Clustering Model",
    "section": "",
    "text": "Clustering Model is one of the unsupervised machine learning model. It groups “unlabelled” data with its own properties which are decided by the model. Since the clustering is unsupervised learning, the data gonna automatically be grouped by the model.\nFor example, when we provide pictures of fruits that contain apple, banana, and orange, the clustering model groups them with three labels defined with their properties like colors, shapes, or something we don’t know."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#actual-example",
    "href": "posts/Clustering/Clustering.html#actual-example",
    "title": "What is Clustering Model",
    "section": "Actual Example",
    "text": "Actual Example\nWe gonna deep dive into the realistic example with the dataset of customers of mall. The input dataset is the information of 200 customers of the mall which contains Genre, Age, Annual Income, and Spending score. The spending score ranges between 1 to 100 that implies how customers spend on the mall.\n\nImporting the libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nImporting the dataset\n\ndataset = pd.read_csv('../dataset/Mall_Customers.csv')\ndataset\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n...\n...\n...\n...\n...\n...\n\n\n195\n196\nFemale\n35\n120\n79\n\n\n196\n197\nFemale\n45\n126\n28\n\n\n197\n198\nMale\n32\n126\n74\n\n\n198\n199\nMale\n32\n137\n18\n\n\n199\n200\nMale\n30\n137\n83\n\n\n\n\n200 rows × 5 columns\n\n\n\n\nX = dataset.iloc[:, [3, 4]].values\n\nNow, we gonna use only annual income and spending score of the dataset. Since we would build 2D clustering graph, we only use these two variables to X.\n\n\nUsing Elbow Function\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n\n\n\nIn the previous elbow method graph, there is a point where the graph rapidly changes from it. It is called “Elbow Point.” The number of clusters from the elbow point will be the \\(K\\) number.\nIn this example, the \\(K\\) is 5.\n\n\nTraining the K-means model\n\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\ny = kmeans.fit_predict(X)\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n\n\nVisualising the results\n\nplt.scatter(X[y == 0, 0], X[y == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y == 2, 0], X[y == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y == 3, 0], X[y == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y == 4, 0], X[y == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()\n\n\n\n\nAs shown above, we successfully group unlabelled dataset into five clusters. Each cluster is divided with its own color. The big yellow points in the graph are centroids of each clusters.\nBy running K-means clustering model, we can group customers into five different clusters. The manager of mall can use this information for whom they want to target."
  },
  {
    "objectID": "posts/Regressor/RegressorModel.html",
    "href": "posts/Regressor/RegressorModel.html",
    "title": "What is Regressor Model",
    "section": "",
    "text": "Linear Regression model is a supervised Machine Learning model. It predicts a dependent variable, y, based on an independent variable, X. The relation between X and y becomes a straight line to predict future data point. Because of these trait, it is best to predict continuous data set."
  },
  {
    "objectID": "posts/Regressor/RegressorModel.html#types-of-linear-regression-model",
    "href": "posts/Regressor/RegressorModel.html#types-of-linear-regression-model",
    "title": "What is Regressor Model",
    "section": "Types of Linear Regression Model",
    "text": "Types of Linear Regression Model\nThere are many types of linear regression model.\n\n1. Simple Linear Regression\nSimple Linear Regression is a basic machine learning model with only two variables using a straight line. In the equation below, y is a dependent variable, alpha is y-interccept, beta is slope, and X is a independent variable.\n\\[\ny=\\alpha +\\beta X\n\\]\nLet’s check with actual example.\n\nImporting the libraries\n\nimport numpy as np\nimport pandas as pd\n\n\n\nImporting the dataset\nOur dataset is Salary Data which contains the average salary information based on years of experience. In this dataset, our independent variable, X, is a years of experience and our independent variable, y, is a salary.\nLet’s think about a simple question. Can we predict someone’s salary based on his years of experience? Assume that you try to find an employee to work on your company and finally see a suitable candidate who already has work experience of 3.5 years. How much salary you need to pay for him?\nLet’s use our simple linear regression model to find the suitable salary for him.\n\ndataset1 = pd.read_csv('../dataset/Salary_Data.csv')\ndataset1\n\n\n\n\n\n\n\n\nYearsExperience\nSalary\n\n\n\n\n0\n1.1\n39343.0\n\n\n1\n1.3\n46205.0\n\n\n2\n1.5\n37731.0\n\n\n3\n2.0\n43525.0\n\n\n4\n2.2\n39891.0\n\n\n5\n2.9\n56642.0\n\n\n6\n3.0\n60150.0\n\n\n7\n3.2\n54445.0\n\n\n8\n3.2\n64445.0\n\n\n9\n3.7\n57189.0\n\n\n10\n3.9\n63218.0\n\n\n11\n4.0\n55794.0\n\n\n12\n4.0\n56957.0\n\n\n13\n4.1\n57081.0\n\n\n14\n4.5\n61111.0\n\n\n15\n4.9\n67938.0\n\n\n16\n5.1\n66029.0\n\n\n17\n5.3\n83088.0\n\n\n18\n5.9\n81363.0\n\n\n19\n6.0\n93940.0\n\n\n20\n6.8\n91738.0\n\n\n21\n7.1\n98273.0\n\n\n22\n7.9\n101302.0\n\n\n23\n8.2\n113812.0\n\n\n24\n8.7\n109431.0\n\n\n25\n9.0\n105582.0\n\n\n26\n9.5\n116969.0\n\n\n27\n9.6\n112635.0\n\n\n28\n10.3\n122391.0\n\n\n29\n10.5\n121872.0\n\n\n\n\n\n\n\n\nX = dataset1.iloc[:, :-1].values\ny = dataset1.iloc[:, -1].values\n\nNow, we split our dataset into test set and train set. We use sklearn train_test_split to manually split dataset. The size of test set gonna be one third of dataset.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0)\n\n\n\nTraining the Simple Linear Regression model\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nNow, we have our simple linear regression model that is fitted with our training set. We can predict y value by using this regressor model to predict with our test set.\n\ny_pred = regressor.predict(X_test)\n\n\n\nVisualising the results\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X_train, y_train, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (Training set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\nThis is our training set result by simple linear regression model. As we seen above, the red scattered points are actual dataset of our training set, and blue straight line is our simple linear equation driven from our model.\nWith this equation and graph, we can predict our test set also.\n\nplt.scatter(X_test, y_test, color = 'red')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue')\nplt.title('Salary vs Experience (Test set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\nLet’s go back to our question. Can we now determine how much salary for the 3.5 years of experience employee?\n\ny_salary = regressor.predict([[3.5]])\nprint(y_salary)\n\n[59526.99079496]\n\n\nOur regressor model determines to pay him almost 59500 dollar which seems reasonable to both you and him.\n\n\n\n2. Multiple Linear Regression\nNow, let’s move on to the other linear regression model, Multiple Linear Regression.\nMultiple Linear Regression model predicts the relationship between a dependent variable and “two or more” independent variables using a straight line. In the equation below, \\(y\\) is a dependent variable that model gonna predict and \\(X_1\\) ~ \\(X_n\\) are independent variables to make a straight line.\n\\[\ny=\\alpha +\\beta_1 X_1 +\\beta_2 X_2 +... + \\beta_n X_n\n\\]\nLet’s move on to the actual example.\n\nImporting the dataset\nSince we already implemented libraries previously, we only need to import the dataset. Our dataset for the multiple linear regression model is the profits of 50 startup companies with their strategies of R&D spend, Administration, Marketing spend, and state. By generating a straight line with these dataset, we can predict a new company’s profit with its strategy.\n\ndataset2 = pd.read_csv('../dataset/50_Startups.csv')\ndataset2\n\n\n\n\n\n\n\n\nR&D Spend\nAdministration\nMarketing Spend\nState\nProfit\n\n\n\n\n0\n165349.20\n136897.80\n471784.10\nNew York\n192261.83\n\n\n1\n162597.70\n151377.59\n443898.53\nCalifornia\n191792.06\n\n\n2\n153441.51\n101145.55\n407934.54\nFlorida\n191050.39\n\n\n3\n144372.41\n118671.85\n383199.62\nNew York\n182901.99\n\n\n4\n142107.34\n91391.77\n366168.42\nFlorida\n166187.94\n\n\n5\n131876.90\n99814.71\n362861.36\nNew York\n156991.12\n\n\n6\n134615.46\n147198.87\n127716.82\nCalifornia\n156122.51\n\n\n7\n130298.13\n145530.06\n323876.68\nFlorida\n155752.60\n\n\n8\n120542.52\n148718.95\n311613.29\nNew York\n152211.77\n\n\n9\n123334.88\n108679.17\n304981.62\nCalifornia\n149759.96\n\n\n10\n101913.08\n110594.11\n229160.95\nFlorida\n146121.95\n\n\n11\n100671.96\n91790.61\n249744.55\nCalifornia\n144259.40\n\n\n12\n93863.75\n127320.38\n249839.44\nFlorida\n141585.52\n\n\n13\n91992.39\n135495.07\n252664.93\nCalifornia\n134307.35\n\n\n14\n119943.24\n156547.42\n256512.92\nFlorida\n132602.65\n\n\n15\n114523.61\n122616.84\n261776.23\nNew York\n129917.04\n\n\n16\n78013.11\n121597.55\n264346.06\nCalifornia\n126992.93\n\n\n17\n94657.16\n145077.58\n282574.31\nNew York\n125370.37\n\n\n18\n91749.16\n114175.79\n294919.57\nFlorida\n124266.90\n\n\n19\n86419.70\n153514.11\n0.00\nNew York\n122776.86\n\n\n20\n76253.86\n113867.30\n298664.47\nCalifornia\n118474.03\n\n\n21\n78389.47\n153773.43\n299737.29\nNew York\n111313.02\n\n\n22\n73994.56\n122782.75\n303319.26\nFlorida\n110352.25\n\n\n23\n67532.53\n105751.03\n304768.73\nFlorida\n108733.99\n\n\n24\n77044.01\n99281.34\n140574.81\nNew York\n108552.04\n\n\n25\n64664.71\n139553.16\n137962.62\nCalifornia\n107404.34\n\n\n26\n75328.87\n144135.98\n134050.07\nFlorida\n105733.54\n\n\n27\n72107.60\n127864.55\n353183.81\nNew York\n105008.31\n\n\n28\n66051.52\n182645.56\n118148.20\nFlorida\n103282.38\n\n\n29\n65605.48\n153032.06\n107138.38\nNew York\n101004.64\n\n\n30\n61994.48\n115641.28\n91131.24\nFlorida\n99937.59\n\n\n31\n61136.38\n152701.92\n88218.23\nNew York\n97483.56\n\n\n32\n63408.86\n129219.61\n46085.25\nCalifornia\n97427.84\n\n\n33\n55493.95\n103057.49\n214634.81\nFlorida\n96778.92\n\n\n34\n46426.07\n157693.92\n210797.67\nCalifornia\n96712.80\n\n\n35\n46014.02\n85047.44\n205517.64\nNew York\n96479.51\n\n\n36\n28663.76\n127056.21\n201126.82\nFlorida\n90708.19\n\n\n37\n44069.95\n51283.14\n197029.42\nCalifornia\n89949.14\n\n\n38\n20229.59\n65947.93\n185265.10\nNew York\n81229.06\n\n\n39\n38558.51\n82982.09\n174999.30\nCalifornia\n81005.76\n\n\n40\n28754.33\n118546.05\n172795.67\nCalifornia\n78239.91\n\n\n41\n27892.92\n84710.77\n164470.71\nFlorida\n77798.83\n\n\n42\n23640.93\n96189.63\n148001.11\nCalifornia\n71498.49\n\n\n43\n15505.73\n127382.30\n35534.17\nNew York\n69758.98\n\n\n44\n22177.74\n154806.14\n28334.72\nCalifornia\n65200.33\n\n\n45\n1000.23\n124153.04\n1903.93\nNew York\n64926.08\n\n\n46\n1315.46\n115816.21\n297114.46\nFlorida\n49490.75\n\n\n47\n0.00\n135426.92\n0.00\nCalifornia\n42559.73\n\n\n48\n542.05\n51743.15\n0.00\nNew York\n35673.41\n\n\n49\n0.00\n116983.80\n45173.06\nCalifornia\n14681.40\n\n\n\n\n\n\n\n\nX = dataset2.iloc[:, :-1].values\ny = dataset2.iloc[:, -1].values\n\n\n\nEncoding categorical data\nIn the dataset, there is a “State” data which consists of three states: New York, California, and Florida. Since a machine learning model only takes numeric data, we need to change this string data into numeric data. Sklearn provides ColumnTransformer and OneHotEncoder libraries to handle this data encoding.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n\n\nprint(X)\n\n[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n [0.0 0.0 1.0 86419.7 153514.11 0.0]\n [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n [1.0 0.0 0.0 0.0 135426.92 0.0]\n [0.0 0.0 1.0 542.05 51743.15 0.0]\n [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n\n\nAs it shown above, each state data is converted into three columns data.\nCalifornia is converted into “1.0 0.0 0.0”\nFlorida is converted into “0.0 1.0 0.0”\nNew York is converted into “0.0 0.0 1.0”\nSo that, our ML model can recognize each states information with these numeric data.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n\n[[103015.2  103282.38]\n [132582.28 144259.4 ]\n [132447.74 146121.95]\n [ 71976.1   77798.83]\n [178537.48 191050.39]\n [116161.24 105008.31]\n [ 67851.69  81229.06]\n [ 98791.73  97483.56]\n [113969.44 110352.25]\n [167921.07 166187.94]]\n\n\nNow, similar with simple linear regression, we can predict our test set with multiple linear regression model."
  }
]