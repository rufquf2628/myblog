{"title":"What is Classfication Model","markdown":{"yaml":{"title":"What is Classfication Model","author":"Hangyul Kim","date":"11/27/2023","image":"knn.png","format":{"html":{"code-fold":false}},"jupyter":"python3"},"headingText":"K-Nearest Neighbors algorithm","containsRefs":false,"markdown":"\n\n\nClassification is a supervised machine learning model that can predict the correct \"label\" of a given input data. Model is fully trained with the given dataset and corresponding label to it.\n\nIn this section, we build a model that can predict that a customer gonna purchase our product with its age and estimated salary. So, in this model, age and estimated salary gonna be an input data, and purchase gonna be a label.\n\n\n\nK-Nearest Neighbors (KNN) is one of the most basic classification algorithm. In the classification phase, a user defines a hyper-parameter \"k\" which is the number of nearest data of each points. Simply, the model predicts which label is assigned to the input data by looking \"k\" nearest points of the input data.\n\n![KNN.png](knn.png)\n\nIn the figure above, k is 5. So, the new data point looks for 5 nearest points and predicts that is is blue label.\n\n#### Importing the libraries and dataset\n\n```{python}\nimport numpy as np\nimport pandas as pd\n```\n\n```{python}\ndataset = pd.read_csv('../dataset/Social_Network_Ads.csv')\ndataset\n```\n\nAs stated above, our dataset, \"Social Network Ads\" contains 400 data of customers age, estimated salary, and wheather they purchased or not.\n\nNow, we split our dataset into train and test set as we done in regression model.\n\n```{python}\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n```\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)\n```\n\n#### Feature Scaling\n\nAs we seen in dataset, we have two input data includes age and estimated salary. Although age is ranged between 10 to 90, estimated salary is ranged between 10000 to 40000; the scale of two inputs are different.\n\nSo, we need to do feature scaling of these data into the range between [-2.0 to 2.0]. StandardScaler library in sklearn will be helpful to handle this.\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n```\n\n#### Training the KNN model\n\nNow, we train our KNN model with training dataset.\n\n```{python}\n#| tags: []\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n```\n\n```{python}\nprint(classifier.predict(sc.transform([[30,87000]])))\n```\n\nWith our trained KNN model, we can predict a specific customer whose age is 30 and estimated salary is 87000 gonna purchase our product. Our model predicts that he don't purchase our product.\n\n#### Making the Confusion Matrix\n\nAlso, we can see how accurate our model is with the confusion matrix provided by sklearn.\n\n```{python}\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)\n```\n\nBy compareing our predicted test set result and actual result, we can get a accuracy score.\n\nOur model shows that it is 93 percent correct in our test set.\n\n#### Visualising the results\n\n```{python}\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n```\n\n```{python}\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n```\n\n","srcMarkdownNoYaml":"\n\n\nClassification is a supervised machine learning model that can predict the correct \"label\" of a given input data. Model is fully trained with the given dataset and corresponding label to it.\n\nIn this section, we build a model that can predict that a customer gonna purchase our product with its age and estimated salary. So, in this model, age and estimated salary gonna be an input data, and purchase gonna be a label.\n\n\n## K-Nearest Neighbors algorithm\n\nK-Nearest Neighbors (KNN) is one of the most basic classification algorithm. In the classification phase, a user defines a hyper-parameter \"k\" which is the number of nearest data of each points. Simply, the model predicts which label is assigned to the input data by looking \"k\" nearest points of the input data.\n\n![KNN.png](knn.png)\n\nIn the figure above, k is 5. So, the new data point looks for 5 nearest points and predicts that is is blue label.\n\n#### Importing the libraries and dataset\n\n```{python}\nimport numpy as np\nimport pandas as pd\n```\n\n```{python}\ndataset = pd.read_csv('../dataset/Social_Network_Ads.csv')\ndataset\n```\n\nAs stated above, our dataset, \"Social Network Ads\" contains 400 data of customers age, estimated salary, and wheather they purchased or not.\n\nNow, we split our dataset into train and test set as we done in regression model.\n\n```{python}\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n```\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)\n```\n\n#### Feature Scaling\n\nAs we seen in dataset, we have two input data includes age and estimated salary. Although age is ranged between 10 to 90, estimated salary is ranged between 10000 to 40000; the scale of two inputs are different.\n\nSo, we need to do feature scaling of these data into the range between [-2.0 to 2.0]. StandardScaler library in sklearn will be helpful to handle this.\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n```\n\n#### Training the KNN model\n\nNow, we train our KNN model with training dataset.\n\n```{python}\n#| tags: []\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n```\n\n```{python}\nprint(classifier.predict(sc.transform([[30,87000]])))\n```\n\nWith our trained KNN model, we can predict a specific customer whose age is 30 and estimated salary is 87000 gonna purchase our product. Our model predicts that he don't purchase our product.\n\n#### Making the Confusion Matrix\n\nAlso, we can see how accurate our model is with the confusion matrix provided by sklearn.\n\n```{python}\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)\n```\n\nBy compareing our predicted test set result and actual result, we can get a accuracy score.\n\nOur model shows that it is 93 percent correct in our test set.\n\n#### Visualising the results\n\n```{python}\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n```\n\n```{python}\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"ClassificationModel.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"theme.css","title-block-banner":false,"author":"Hangyul Kim","page-layout":"article","title":"What is Classfication Model","date":"11/27/2023","image":"knn.png","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}